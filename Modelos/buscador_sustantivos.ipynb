{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importaciones y descargas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 22.3MB/s]                    \n",
      "2025-03-19 19:19:53 INFO: Downloaded file to C:\\Users\\alumnoBIGDATA\\stanza_resources\\resources.json\n",
      "2025-03-19 19:19:53 INFO: Downloading these customized packages for language: es (Spanish)...\n",
      "=====================================\n",
      "| Processor       | Package         |\n",
      "-------------------------------------\n",
      "| tokenize        | ancora          |\n",
      "| mwt             | ancora          |\n",
      "| pos             | ancora_charlm   |\n",
      "| lemma           | ancora_nocharlm |\n",
      "| pretrain        | conll17         |\n",
      "| backward_charlm | newswiki        |\n",
      "| forward_charlm  | newswiki        |\n",
      "=====================================\n",
      "\n",
      "2025-03-19 19:19:53 INFO: File exists: C:\\Users\\alumnoBIGDATA\\stanza_resources\\es\\tokenize\\ancora.pt\n",
      "2025-03-19 19:19:53 INFO: File exists: C:\\Users\\alumnoBIGDATA\\stanza_resources\\es\\mwt\\ancora.pt\n",
      "2025-03-19 19:19:53 INFO: File exists: C:\\Users\\alumnoBIGDATA\\stanza_resources\\es\\pos\\ancora_charlm.pt\n",
      "2025-03-19 19:19:53 INFO: File exists: C:\\Users\\alumnoBIGDATA\\stanza_resources\\es\\lemma\\ancora_nocharlm.pt\n",
      "2025-03-19 19:19:53 INFO: File exists: C:\\Users\\alumnoBIGDATA\\stanza_resources\\es\\pretrain\\conll17.pt\n",
      "2025-03-19 19:19:54 INFO: File exists: C:\\Users\\alumnoBIGDATA\\stanza_resources\\es\\backward_charlm\\newswiki.pt\n",
      "2025-03-19 19:19:54 INFO: File exists: C:\\Users\\alumnoBIGDATA\\stanza_resources\\es\\forward_charlm\\newswiki.pt\n",
      "2025-03-19 19:19:54 INFO: Finished downloading models and saved to C:\\Users\\alumnoBIGDATA\\stanza_resources\n",
      "2025-03-19 19:19:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 16.2MB/s]                    \n",
      "2025-03-19 19:19:54 INFO: Downloaded file to C:\\Users\\alumnoBIGDATA\\stanza_resources\\resources.json\n",
      "2025-03-19 19:19:54 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-03-19 19:19:54 WARNING: GPU requested, but is not available!\n",
      "2025-03-19 19:19:54 INFO: Using device: cpu\n",
      "2025-03-19 19:19:54 INFO: Loading: tokenize\n",
      "2025-03-19 19:19:57 INFO: Loading: mwt\n",
      "2025-03-19 19:19:57 INFO: Loading: pos\n",
      "2025-03-19 19:19:59 INFO: Loading: lemma\n",
      "2025-03-19 19:20:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "import stanza\n",
    "from whoosh import index\n",
    "from whoosh.fields import Schema, TEXT\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# Descargar recursos de stanza\n",
    "stanza.download('es',package='ancora', processors='tokenize,mwt,pos,lemma', verbose=True) \n",
    "\n",
    "# Inicializar el pipeline de stanza\n",
    "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma',lang='es',use_gpu=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Leer fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_fichero(ruta_fichero):\n",
    "    with open(ruta_fichero, \"r\", encoding='utf-8') as f:\n",
    "        contenido = f.read()\n",
    "    return contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extraer secciones, limpiar texto y extraer sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_secciones(contenido):\n",
    "    secciones = {}\n",
    "    # Usar expresiones regulares para encontrar las secciones\n",
    "    titulo_match = re.search(r'Titulo(.*?)(?=Noticia|Resumen|$)', contenido, re.DOTALL)\n",
    "    noticia_match = re.search(r'Noticia(.*?)(?=Resumen|$)', contenido, re.DOTALL)\n",
    "    resumen_match = re.search(r'Resumen(.*?)(?=$)', contenido, re.DOTALL)\n",
    "\n",
    "    if titulo_match:\n",
    "        secciones['titulo'] = titulo_match.group(1).strip()\n",
    "    if noticia_match:\n",
    "        secciones['noticia'] = noticia_match.group(1).strip()\n",
    "    if resumen_match:\n",
    "        secciones['resumen'] = resumen_match.group(1).strip()\n",
    "\n",
    "    return secciones\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r'\\s+', ' ', texto)  # Reemplazar múltiples espacios por uno solo\n",
    "    texto = re.sub(r'[.,;!?]', '', texto)  # Eliminar puntuación\n",
    "    return texto\n",
    "\n",
    "def extraer_sustantivos(texto):\n",
    "    doc = stNLP(texto)\n",
    "    sustantivos = set()  # Usamos set para evitar duplicados\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.pos == 'NOUN' or word.pos == 'PROPN':\n",
    "                sustantivos.add(word.text)\n",
    "\n",
    "    return sustantivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Buscador woosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Definir el esquema de Whoosh\n",
    "esquema = Schema(seccion=TEXT(stored=True), sustantivos=TEXT(stored=True))\n",
    "\n",
    "def crear_indice(sustantivos):\n",
    "    if not os.path.exists(\"indice_sustantivos\"):\n",
    "        os.mkdir(\"indice_sustantivos\")\n",
    "    \n",
    "    index.create_in(\"indice_sustantivos\", esquema)\n",
    "    indice = index.open_dir(\"indice_sustantivos\")\n",
    "    writer = indice.writer()\n",
    "    \n",
    "    for seccion, palabras in sustantivos.items():\n",
    "        writer.add_document(seccion=seccion, sustantivos=\" \".join(palabras))\n",
    "    \n",
    "    writer.commit()\n",
    "\n",
    "def buscar_sustantivo(seccion, sustantivo):\n",
    "    indice = index.open_dir(\"indice_sustantivos\")\n",
    "    parser = QueryParser(\"sustantivos\", schema=esquema)\n",
    "    consulta = parser.parse(sustantivo)\n",
    "    \n",
    "    with indice.searcher() as buscador:\n",
    "        resultados = buscador.search(consulta)\n",
    "        for resultado in resultados:\n",
    "            if resultado[\"seccion\"] == seccion:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sustantivos en titulo: día, muertes\n",
      "\n",
      "Sustantivos en noticia: virus, final, pandemia, fase, balance, daños, expertos, tiempo, registros, semanas, España\n",
      "\n",
      "Sustantivos en resumen: capacidad, serie, hospitalizados, registro, personas, infección\n"
     ]
    }
   ],
   "source": [
    "contenido = leer_fichero(\"./ejemplo1.txt\")\n",
    "secciones = extraer_secciones(contenido)\n",
    "sustantivos = {}\n",
    "\n",
    "for seccion, texto in secciones.items():\n",
    "    texto_limpio = limpiar_texto(texto)\n",
    "    sustantivos[seccion] = extraer_sustantivos(texto_limpio)\n",
    "\n",
    "# Imprimir resultado\n",
    "for seccion, palabras in sustantivos.items():\n",
    "    print(f\"\\nSustantivos en {seccion}: {', '.join(palabras) if palabras else 'No hay sustantivos'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Buscador de sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El sustantivo 'día' SÍ está en la sección 'titulo'.\n"
     ]
    }
   ],
   "source": [
    "# Crear el índice de Whoosh\n",
    "crear_indice(sustantivos)\n",
    "\n",
    "# Interacción con el usuario\n",
    "seccion_usuario = input(\"En qué sección quieres buscar (titulo, noticia, resumen)?: \").strip().lower()\n",
    "sustantivo_usuario = input(\"Qué sustantivo quieres encontrar?: \").strip().lower()\n",
    "\n",
    "if buscar_sustantivo(seccion_usuario, sustantivo_usuario):\n",
    "    print(f\"El sustantivo '{sustantivo_usuario}' SÍ está en la sección '{seccion_usuario}'.\")\n",
    "else:\n",
    "    print(f\"El sustantivo '{sustantivo_usuario}' NO está en la sección '{seccion_usuario}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
